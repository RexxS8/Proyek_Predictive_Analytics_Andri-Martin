# -*- coding: utf-8 -*-
"""Regression-Daatset Flood.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nZtZN4m6yjmk6XulmjPduyeNvbcFukSY

# **1. Perkenalan Dataset**

Dataset ini bertujuan untuk membantu membangun model regresi guna memprediksi kemungkinan banjir **(FloodProbability)** berdasarkan 20 variabel independen yang mencerminkan faktor lingkungan dan sosial-ekonomi seperti:

* MonsoonIntensity
* Deforestation
* Urbanization
* ClimateChange

Semua variabel bertipe numerik **(int64)** dan tidak terdapat nilai yang hilang, sehingga dataset ini sangat cocok digunakan untuk model regresi tanpa preprocessing berat.

# **2. Import Library**

Pada tahap ini, Anda perlu mengimpor beberapa pustaka (library) Python yang dibutuhkan untuk analisis data dan pembangunan model machine learning.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

"""# **3. Memuat Dataset**

Pada tahap ini, Anda perlu memuat dataset ke dalam notebook. Jika dataset dalam format CSV, Anda bisa menggunakan pustaka pandas untuk membacanya. Pastikan untuk mengecek beberapa baris awal dataset untuk memahami strukturnya dan memastikan data telah dimuat dengan benar.

Jika dataset berada di Google Drive, pastikan Anda menghubungkan Google Drive ke Colab terlebih dahulu. Setelah dataset berhasil dimuat, langkah berikutnya adalah memeriksa kesesuaian data dan siap untuk dianalisis lebih lanjut.
"""

# Load dataset dari CSV
df = pd.read_csv("flood.csv")

# Menampilkan beberapa baris awal
df.head()

"""# **4. Exploratory Data Analysis (EDA)**

Pada tahap ini, Anda akan melakukan **Exploratory Data Analysis (EDA)** untuk memahami karakteristik dataset. EDA bertujuan untuk:

1. **Memahami Struktur Data**
   - Tinjau jumlah baris dan kolom dalam dataset.  
   - Tinjau jenis data di setiap kolom (numerikal atau kategorikal).

2. **Menangani Data yang Hilang**  
   - Identifikasi dan analisis data yang hilang (*missing values*). Tentukan langkah-langkah yang diperlukan untuk menangani data yang hilang, seperti pengisian atau penghapusan data tersebut.

3. **Analisis Distribusi dan Korelasi**  
   - Analisis distribusi variabel numerik dengan statistik deskriptif dan visualisasi seperti histogram atau boxplot.  
   - Periksa hubungan antara variabel menggunakan matriks korelasi atau scatter plot.

4. **Visualisasi Data**  
   - Buat visualisasi dasar seperti grafik distribusi dan diagram batang untuk variabel kategorikal.  
   - Gunakan heatmap atau pairplot untuk menganalisis korelasi antar variabel.

Tujuan dari EDA adalah untuk memperoleh wawasan awal yang mendalam mengenai data dan menentukan langkah selanjutnya dalam analisis atau pemodelan.

**4.1. Memahami Struktur Data**
"""

print(df.info())  # Struktur dataset

df.describe()  # Statistik deskriptif

df.isnull().sum()  # Mengecek missing values

# Distribusi target
plt.figure(figsize=(8, 5))
sns.histplot(df['FloodProbability'], bins=30, kde=True)
plt.title("Distribusi FloodProbability")
plt.show()

# Korelasi antar fitur
plt.figure(figsize=(16, 12))
sns.heatmap(df.corr(), annot=False, cmap='coolwarm')
plt.title("Heatmap Korelasi Antar Variabel")
plt.show()

# Boxplot fitur yang paling relevan
features = ['MonsoonIntensity', 'Urbanization', 'Deforestation', 'ClimateChange']
for col in features:
    plt.figure(figsize=(6, 4))
    sns.boxplot(x=df[col])
    plt.title(f"Boxplot: {col}")
    plt.show()

"""# **5. Data Preprocessing**

Pada tahap ini, data preprocessing adalah langkah penting untuk memastikan kualitas data sebelum digunakan dalam model machine learning. Data mentah sering kali mengandung nilai kosong, duplikasi, atau rentang nilai yang tidak konsisten, yang dapat memengaruhi kinerja model. Oleh karena itu, proses ini bertujuan untuk membersihkan dan mempersiapkan data agar analisis berjalan optimal.

Berikut adalah tahapan-tahapan yang bisa dilakukan, tetapi **tidak terbatas** pada:
1. Menghapus atau Menangani Data Kosong (Missing Values)
2. Menghapus Data Duplikat
3. Normalisasi atau Standarisasi Fitur
4. Deteksi dan Penanganan Outlier
5. Encoding Data Kategorikal
6. Binning (Pengelompokan Data)

Cukup sesuaikan dengan karakteristik data yang kamu gunakan yah.
"""

# Menghapus missing values
df.duplicated().sum()
df = df.drop_duplicates()

"""Standarisasi / Normalisasi"""

X = df.drop(columns='FloodProbability')
y = df['FloodProbability']

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_scaled

"""# **6. Pembangunan Model Clustering**

## **a. Pembangunan Model Clustering**

Pada tahap ini, Anda membangun model clustering dengan memilih algoritma yang sesuai untuk mengelompokkan data berdasarkan kesamaan. Berikut adalah **rekomendasi** tahapannya.
1. Pilih algoritma clustering yang sesuai.
2. Latih model dengan data menggunakan algoritma tersebut.
"""

X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42
)

lr_model = LinearRegression()
lr_model.fit(X_train, y_train)
y_pred_lr = lr_model.predict(X_test)

# Evaluasi
print("Linear Regression:")
print("MAE:", mean_absolute_error(y_test, y_pred_lr))
print("MSE:", mean_squared_error(y_test, y_pred_lr))
print("RMSE:", np.sqrt(mean_squared_error(y_test, y_pred_lr)))
print("R2 Score:", r2_score(y_test, y_pred_lr))

rf_model = RandomForestRegressor(random_state=42)
rf_model.fit(X_train, y_train)
y_pred_rf = rf_model.predict(X_test)

# Evaluasi
print("\nRandom Forest Regressor:")
print("MAE:", mean_absolute_error(y_test, y_pred_rf))
print("MSE:", mean_squared_error(y_test, y_pred_rf))
print("RMSE:", np.sqrt(mean_squared_error(y_test, y_pred_rf)))
print("R2 Score:", r2_score(y_test, y_pred_rf))

import matplotlib.pyplot as plt

importances = rf_model.feature_importances_
features = df.drop(columns='FloodProbability').columns
indices = np.argsort(importances)[::-1]

plt.figure(figsize=(10, 6))
sns.barplot(x=importances[indices], y=features[indices])
plt.title("Feature Importance (Random Forest)")
plt.show()

"""# **7. Tuning Model**"""

from sklearn.model_selection import GridSearchCV

# Parameter grid
param_grid = {
    'n_estimators': [50, 100, 150],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10]
}

# Inisiasi model
grid_search = GridSearchCV(
    estimator=RandomForestRegressor(random_state=42),
    param_grid=param_grid,
    cv=5,
    scoring='r2',
    n_jobs=-1,
    verbose=1
)

# Fit ke data training
grid_search.fit(X_train, y_train)

# Menampilkan parameter terbaik
print("Best Parameters:", grid_search.best_params_)

# Evaluasi model terbaik
best_rf = grid_search.best_estimator_
y_pred_best_rf = best_rf.predict(X_test)

print("Random Forest (Tuned):")
print("MAE:", mean_absolute_error(y_test, y_pred_best_rf))
print("MSE:", mean_squared_error(y_test, y_pred_best_rf))
print("RMSE:", np.sqrt(mean_squared_error(y_test, y_pred_best_rf)))
print("R2 Score:", r2_score(y_test, y_pred_best_rf))

import matplotlib.pyplot as plt

importances = best_rf.feature_importances_
features = df.drop(columns='FloodProbability').columns
indices = np.argsort(importances)[::-1]

plt.figure(figsize=(10, 6))
sns.barplot(x=importances[indices], y=features[indices])
plt.title("Feature Importance dari Random Forest (Tuned)")
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.tight_layout()
plt.show()

"""# **8. Interpretasi dan Insight dari Model**

## 8.1. Hasil Linear Regression

Model Linear Regression menghasilkan skor R² mendekati 1.0, yang menandakan model mampu menjelaskan hampir seluruh variasi dalam data. Hal ini bisa disebabkan karena:

- Korelasi antar variabel input dan target sangat tinggi.
- Data tidak mengandung noise (karena sudah bersih dan tidak ada outlier/missing).
- Hubungan antar variabel bersifat linear.

Namun, perlu waspada terhadap overfitting karena nilai R² yang terlalu tinggi dapat mengindikasikan model terlalu "mengikuti" data. Oleh karena itu, validasi silang atau eksperimen model lain seperti Random Forest menjadi penting.

Sebagai pembanding, model Random Forest Regressor yang digunakan menghasilkan skor evaluasi sebagai berikut:
### Sebelum tuning:
- MAE: 0.02047
- MSE: 0.00067
- RMSE: 0.02591
- R² Score: 0.7305
### Setelah tuning (Grid SearchCV):
- MAE: 0.02037
- MSE: 0.00067
- RMSE: 0.02582
- R² Score: 0.7324

## Hasilnya:
- Model Random Forest memiliki performa prediksi yang cukup baik, dengan R² sekitar 0.73, menunjukkan bahwa model ini mampu menjelaskan sekitar 73% variasi dalam data.

- Perbedaan performa sebelum dan sesudah tuning sangat kecil, namun tetap menunjukkan adanya peningkatan meskipun marginal. Ini menandakan model cukup stabil.

- MAE dan RMSE yang rendah menunjukkan bahwa kesalahan prediksi juga tergolong kecil.

- Dibandingkan dengan Linear Regression, Random Forest lebih robust terhadap data non-linear dan potensi overfitting, serta memberikan insight tambahan dari feature importance.


## 8.2. Feature Importance dari Random Forest

Berikut adalah 5 fitur terpenting menurut model Random Forest:

1. **TopographyDrainage** – Kemampuan drainase alami yang dipengaruhi oleh topografi suatu wilayah sangat krusial dalam menentukan seberapa cepat air hujan dapat dialirkan atau tertahan. Wilayah dengan kontur yang buruk atau cekungan alami berpotensi menjadi titik genangan, sehingga meningkatkan risiko banjir secara signifikan.
2. **DamsQuality** – Kualitas struktur dan pemeliharaan bendungan sangat memengaruhi pengendalian volume air. Bendungan yang rusak atau tidak dirawat dengan baik berpotensi jebol dan menyebabkan banjir besar. Sebaliknya, bendungan yang baik dapat berfungsi sebagai pengatur aliran air saat hujan lebat.
3. **PoliticalFactors** – Faktor politik seperti korupsi, alokasi anggaran yang tidak tepat, dan lemahnya regulasi menjadi hambatan utama dalam pembangunan serta pemeliharaan infrastruktur pengendalian banjir. Ketidakefisienan birokrasi juga memperburuk mitigasi risiko banjir.
4. **PopulationScore** – Wilayah dengan kepadatan penduduk tinggi cenderung memiliki lebih banyak kerugian saat banjir terjadi. Selain itu, peningkatan jumlah penduduk biasanya diikuti dengan ekspansi urbanisasi yang mengurangi lahan resapan air, memperburuk situasi saat curah hujan ekstrem.
5. **IneffectiveDisasterPreparedness** – Ketidaksiapan dalam menghadapi bencana, seperti tidak adanya sistem peringatan dini, prosedur evakuasi, dan pelatihan masyarakat, memperparah dampak banjir. Fitur ini mencerminkan kurangnya sistem mitigasi yang responsif dan terstruktur.

## 8.3. Rekomendasi Kebijakan

Berdasarkan hasil analisis model dan fitur-fitur paling berpengaruh terhadap probabilitas banjir, berikut beberapa rekomendasi kebijakan yang dapat diterapkan oleh pemerintah dan pemangku kepentingan:

- Perbaiki kualitas dan pemeliharaan bendungan : Lakukan audit rutin dan perbaikan bendungan untuk mengontrol aliran air secara optimal.
- Tingkatkan sistem drainase berbasis topografi :Fokus pada wilayah dengan drainase buruk dan sesuaikan pembangunan dengan kontur alami.
- Perkuat tata kelola dan akuntabilitas politik : Tingkatkan transparansi anggaran dan pemberantasan korupsi dalam proyek infrastruktur.
- Rancang tata ruang berdasarkan kepadatan penduduk : Sediakan jalur evakuasi dan infrastruktur tahan banjir di daerah padat penduduk.
- Perkuat kesiapsiagaan bencana : Bangun sistem peringatan dini, lakukan simulasi rutin, dan bentuk tim tanggap darurat.

## 8.4. Kesimpulan

Model regresi yang dibangun, terutama Random Forest Regressor, berhasil memprediksi probabilitas banjir dengan performa yang baik (R² ≈ 0.73) dan kesalahan prediksi rendah (RMSE ≈ 0.025). Meskipun Linear Regression menunjukkan nilai R² yang sangat tinggi, pendekatan Random Forest lebih dapat diandalkan karena kemampuannya menangkap hubungan non-linear dan memberikan informasi penting mengenai kontribusi masing-masing fitur.

Fitur-fitur seperti TopographyDrainage, DamsQuality, PoliticalFactors, PopulationScore, dan IneffectiveDisasterPreparedness terbukti sangat berpengaruh dalam menentukan risiko banjir. Insight dari model ini dapat dijadikan dasar dalam merancang kebijakan mitigasi banjir yang lebih efektif dan berbasis data.

Dengan demikian, implementasi model prediktif ini memiliki potensi besar sebagai alat bantu pengambilan keputusan berbasis bukti (evidence-based policy) untuk mengurangi risiko dan dampak banjir di masa mendatang.

"""